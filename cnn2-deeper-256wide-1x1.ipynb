{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (0.4.1)\n",
      "Requirement already satisfied: torchvision in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg (0.2.1)\n",
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision) (1.14.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision) (5.2.0)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from torchvision) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.0.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Exception reporting mode: Verbose\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision\n",
    "%xmode Verbose\n",
    "\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "torch.__version__\n",
    "\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(SimpleCNN,self).__init__()\n",
    "    \n",
    "    self.conv1_out_ch = 256\n",
    "    self.conv2_out_ch = 256\n",
    "    self.conv3_out_ch = 256\n",
    "    self.conv4_out_ch = 256\n",
    "    self.conv5_out_ch = 256\n",
    "    self.conv6_out_ch = 256\n",
    "    self.conv7_out_ch = 256\n",
    "    self.conv8_out_ch = 256\n",
    "    self.conv9_out_ch = 256\n",
    "    self.conv10_out_ch = 256\n",
    "    self.conv11_out_ch = 256\n",
    "    self.conv12_out_ch = 256\n",
    "\n",
    "    self.dropout_p = 0.5\n",
    "    self.dropout_p_fc = 0.3\n",
    "    \n",
    "    self.conv1 = torch.nn.Conv2d(3, self.conv1_out_ch, kernel_size=3, stride=1, padding=1)\n",
    "    self.bn1 = torch.nn.BatchNorm2d(self.conv1_out_ch)\n",
    "    self.dropout1 = torch.nn.Dropout(p=self.dropout_p)\n",
    "    \n",
    "    self.conv2 = torch.nn.Conv2d(self.conv1_out_ch, self.conv2_out_ch, kernel_size=3, stride=1, padding=1)\n",
    "    self.bn2 = torch.nn.BatchNorm2d(self.conv2_out_ch)\n",
    "    self.dropout2 = torch.nn.Dropout(p=self.dropout_p)\n",
    "    self.pool2 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    self.conv3 = torch.nn.Conv2d(self.conv2_out_ch, self.conv3_out_ch, kernel_size=3, stride=1, padding=1)\n",
    "    self.bn3 = torch.nn.BatchNorm2d(self.conv3_out_ch)\n",
    "    self.dropout3 = torch.nn.Dropout(p=self.dropout_p)\n",
    "    self.pool3 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    self.conv4 = torch.nn.Conv2d(self.conv3_out_ch, self.conv4_out_ch, kernel_size=3, stride=1, padding=1)\n",
    "    self.bn4 = torch.nn.BatchNorm2d(self.conv4_out_ch)\n",
    "    self.dropout4 = torch.nn.Dropout(p=self.dropout_p)\n",
    "    self.pool4 = torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "    self.conv5 = torch.nn.Conv2d(self.conv4_out_ch, self.conv5_out_ch, kernel_size=3, stride=1, padding=1)\n",
    "    self.bn5 = torch.nn.BatchNorm2d(self.conv5_out_ch)\n",
    "    self.dropout5 = torch.nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "    self.conv6 = torch.nn.Conv2d(self.conv5_out_ch, self.conv6_out_ch, kernel_size=3, stride=1, padding=1)\n",
    "    self.bn6 = torch.nn.BatchNorm2d(self.conv6_out_ch)\n",
    "    self.dropout6 = torch.nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "    self.conv7 = torch.nn.Conv2d(self.conv6_out_ch, self.conv7_out_ch, kernel_size=3, stride=1, padding=1)\n",
    "    self.bn7 = torch.nn.BatchNorm2d(self.conv7_out_ch)\n",
    "    self.dropout7 = torch.nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "    self.conv8 = torch.nn.Conv2d(self.conv7_out_ch, self.conv8_out_ch, kernel_size=3, stride=1, padding=1)\n",
    "    self.bn8 = torch.nn.BatchNorm2d(self.conv8_out_ch)\n",
    "    self.dropout8 = torch.nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "    self.conv9 = torch.nn.Conv2d(self.conv8_out_ch, self.conv9_out_ch, kernel_size=1, stride=1, padding=0)\n",
    "    self.bn9 = torch.nn.BatchNorm2d(self.conv9_out_ch)\n",
    "    self.dropout9 = torch.nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "    self.conv10 = torch.nn.Conv2d(self.conv9_out_ch, self.conv10_out_ch, kernel_size=1, stride=1, padding=0)\n",
    "    self.bn10 = torch.nn.BatchNorm2d(self.conv10_out_ch)\n",
    "    self.dropout10 = torch.nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "    self.conv11 = torch.nn.Conv2d(self.conv10_out_ch, self.conv11_out_ch, kernel_size=1, stride=1, padding=0)\n",
    "    self.bn11 = torch.nn.BatchNorm2d(self.conv11_out_ch)\n",
    "    self.dropout11 = torch.nn.Dropout(p=self.dropout_p)\n",
    "    \n",
    "    self.conv12 = torch.nn.Conv2d(self.conv11_out_ch, self.conv12_out_ch, kernel_size=1, stride=1, padding=0)\n",
    "    self.bn12 = torch.nn.BatchNorm2d(self.conv12_out_ch)\n",
    "    self.dropout12 = torch.nn.Dropout(p=self.dropout_p)\n",
    "\n",
    "    self.fc1 = torch.nn.Linear(self.conv12_out_ch*4*4, 128)\n",
    "    self.dropoutfc1 = torch.nn.Dropout(p=self.dropout_p_fc)\n",
    "    self.fc2 = torch.nn.Linear(128, 10)\n",
    "    \n",
    "  def forward(self,x):\n",
    "    x = F.relu(self.bn1(self.conv1(x)))\n",
    "    x = self.dropout1(x)\n",
    "\n",
    "    x = F.relu(self.bn2(self.conv2(x)))\n",
    "    x = self.dropout2(x)\n",
    "    x = self.pool2(x)\n",
    "    \n",
    "    x = F.relu(self.bn3(self.conv3(x)))\n",
    "    x = self.dropout3(x)\n",
    "    x = self.pool3(x)\n",
    "    \n",
    "    x = F.relu(self.bn4(self.conv4(x)))\n",
    "    x = self.dropout4(x)\n",
    "    x = self.pool4(x)        \n",
    "\n",
    "    x = F.relu(self.bn5(self.conv5(x)))\n",
    "    x = self.dropout5(x)\n",
    "    \n",
    "    x = F.relu(self.bn6(self.conv6(x)))\n",
    "    x = self.dropout6(x)\n",
    "\n",
    "    x = F.relu(self.bn7(self.conv7(x)))\n",
    "    x = self.dropout7(x)\n",
    "\n",
    "    x = F.relu(self.bn8(self.conv8(x)))\n",
    "    x = self.dropout8(x)\n",
    "\n",
    "    x = F.relu(self.bn9(self.conv9(x)))\n",
    "    x = self.dropout9(x)\n",
    "\n",
    "    x = F.relu(self.bn10(self.conv10(x)))\n",
    "    x = self.dropout10(x)\n",
    "\n",
    "    x = F.relu(self.bn11(self.conv11(x)))\n",
    "    x = self.dropout11(x)\n",
    "\n",
    "    x = F.relu(self.bn12(self.conv12(x)))\n",
    "    x = self.dropout12(x)\n",
    "\n",
    "    x = x.view(-1, self.conv12_out_ch*4*4)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(loader, model) :\n",
    "  y_true = []\n",
    "  y_pred = []\n",
    "\n",
    "  count = 0\n",
    "  accuracy_sum = 0\n",
    "\n",
    "  import sys\n",
    "\n",
    "  with torch.no_grad() :\n",
    "    for x, y in loader :\n",
    "      x = x.float().to(device)\n",
    "      y = y.to(device)\n",
    "      model.eval()\n",
    "      outputs = model.forward(x)\n",
    "\n",
    "      outputs = outputs.cpu().numpy()\n",
    "      y_pred = np.argmax(outputs, axis=1)\n",
    "      y = y.cpu().numpy()\n",
    "\n",
    "      match = (y == y_pred).astype('uint8')\n",
    "      accuracy_sum += np.sum(match) \n",
    "      count += len(match)\n",
    "\n",
    "    accuracy = accuracy_sum / count\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "train_set = torchvision.datasets.CIFAR10(root='./cifardata', train=True, download=True, transform=transform)\n",
    "test_set = torchvision.datasets.CIFAR10(root='./cifardata', train=False, download=True, transform=transform)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "#Training\n",
    "n_training_samples=45000\n",
    "train_sampler=SubsetRandomSampler(np.arange(n_training_samples))\n",
    "\n",
    "#Validation\n",
    "n_val_samples = 5000\n",
    "val_sampler = SubsetRandomSampler(np.arange(n_training_samples, n_training_samples + n_val_samples, dtype=np.int64))\n",
    "\n",
    "#Test\n",
    "n_test_samples = 10000\n",
    "test_sampler = SubsetRandomSampler(np.arange(n_test_samples, dtype=np.int64))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=4, sampler=test_sampler, num_workers=2)\n",
    "val_loader = torch.utils.data.DataLoader(train_set, batch_size=128, sampler=val_sampler, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "import time\n",
    "\n",
    "def trainNet(net, batch_size, n_epochs, learning_rate):\n",
    "  print(\"==== HYPERPARAMETERS ====\")\n",
    "  print(\"batch_size=\", batch_size)\n",
    "  print(\"n_epochs=\",n_epochs)\n",
    "  print(\"learning_rate=\",learning_rate)\n",
    "  print(\"=\"*30)\n",
    "  \n",
    "  # Get training data\n",
    "  train_loader = torch.utils.data.DataLoader(train_set, \n",
    "                                             batch_size=batch_size,\n",
    "                                             sampler=train_sampler,\n",
    "                                             num_workers=2)\n",
    "  n_batches = len(train_loader)\n",
    "  \n",
    "  # Create loss and optimizer functions\n",
    "  loss = torch.nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "  \n",
    "  training_start_time = time.time()\n",
    "  \n",
    "  for epoch in range(n_epochs):\n",
    "    running_loss = 0.0\n",
    "    print_every = n_batches\n",
    "    start_time = time.time()\n",
    "    total_train_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(train_loader,0):\n",
    "      inputs, labels = data\n",
    "      inputs, labels = Variable(inputs), Variable(labels)\n",
    "      inputs = inputs.float().to(device)\n",
    "      labels = labels.to(device)\n",
    "      \n",
    "      optimizer.zero_grad()\n",
    "      \n",
    "      net.train()\n",
    "      outputs = net(inputs)\n",
    "      loss_size = loss(outputs, labels)\n",
    "      loss_size.backward()\n",
    "      optimizer.step()\n",
    "      \n",
    "      running_loss += loss_size.data\n",
    "      total_train_loss += loss_size.data\n",
    "      \n",
    "      if (i+1)%print_every == 0:\n",
    "        print(\"Epoch {}, {:d}% \\t train_loss: {:.2f} took {:.2f}s\".format(\n",
    "              epoch+1, int(100*(i+1)/n_batches), running_loss/print_every, time.time()-start_time))\n",
    "        running_loss = 0.0\n",
    "        start_time = time.time()\n",
    "      \n",
    "    # At the end of the epoch, do a pass on the validation set\n",
    "    total_val_loss = 0\n",
    "    for inputs, labels in val_loader:\n",
    "      \n",
    "      inputs, labels = Variable(inputs), Variable(labels)\n",
    "      inputs = inputs.float().to(device)\n",
    "      labels = labels.to(device)\n",
    "      \n",
    "      net.eval()\n",
    "      val_outputs = net(inputs)\n",
    "      val_loss_size = loss(val_outputs, labels)\n",
    "      total_val_loss += val_loss_size.data\n",
    "      \n",
    "    print(\"Validation loss = {:.2f}\".format(total_val_loss/len(val_loader)))\n",
    "          \n",
    "  print(\"Training finished, took {:.2f}s\".format(time.time()-training_start_time))\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== HYPERPARAMETERS ====\n",
      "batch_size= 32\n",
      "n_epochs= 50\n",
      "learning_rate= 0.001\n",
      "==============================\n",
      "Epoch 1, 100% \t train_loss: 1.71 took 62.28s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f601bbcfdd8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 399, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 378, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 151, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 494, in Client\n",
      "    deliver_challenge(c, authkey)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 722, in deliver_challenge\n",
      "    response = connection.recv_bytes(256)        # reject large message\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/multiprocessing/connection.py\", line 383, in _recv\n",
      "    raise EOFError\n",
      "EOFError: \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-89a9b4e13898>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mCNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mCNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrainNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mglobal\u001b[0m \u001b[0;36mtrainNet\u001b[0m \u001b[0;34m= <function trainNet at 0x7f601bbea048>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mCNN\u001b[0m \u001b[0;34m= SimpleCNN(\n  (conv1): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout1): Dropout(p=0.5)\n  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout2): Dropout(p=0.5)\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout3): Dropout(p=0.5)\n  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout4): Dropout(p=0.5)\n  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout5): Dropout(p=0.5)\n  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout6): Dropout(p=0.5)\n  (conv7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout7): Dropout(p=0.5)\n  (conv8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout8): Dropout(p=0.5)\n  (conv9): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  (bn9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout9): Dropout(p=0.5)\n  (conv10): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  (bn10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout10): Dropout(p=0.5)\n  (conv11): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  (bn11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout11): Dropout(p=0.5)\n  (conv12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  (bn12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout12): Dropout(p=0.5)\n  (fc1): Linear(in_features=4096, out_features=128, bias=True)\n  (dropoutfc1): Dropout(p=0.3)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mbatch_size\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mn_epochs\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mlearning_rate\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-0914d2d589d9>\u001b[0m in \u001b[0;36mtrainNet\u001b[0;34m(net=SimpleCNN(\n  (conv1): Conv2d(3, 256, kernel_size...ar(in_features=128, out_features=10, bias=True)\n), batch_size=32, n_epochs=50, learning_rate=0.001)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m       \u001b[0mval_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mval_outputs\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mnet\u001b[0m \u001b[0;34m= SimpleCNN(\n  (conv1): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout1): Dropout(p=0.5)\n  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout2): Dropout(p=0.5)\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout3): Dropout(p=0.5)\n  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout4): Dropout(p=0.5)\n  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout5): Dropout(p=0.5)\n  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout6): Dropout(p=0.5)\n  (conv7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout7): Dropout(p=0.5)\n  (conv8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout8): Dropout(p=0.5)\n  (conv9): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  (bn9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout9): Dropout(p=0.5)\n  (conv10): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  (bn10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout10): Dropout(p=0.5)\n  (conv11): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  (bn11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout11): Dropout(p=0.5)\n  (conv12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  (bn12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout12): Dropout(p=0.5)\n  (fc1): Linear(in_features=4096, out_features=128, bias=True)\n  (dropoutfc1): Dropout(p=0.3)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36minputs\u001b[0m \u001b[0;34m= tensor([[[[-0.6235, -0.5843, -0.5529,  ..., -0.7020, -0.7020, -0.6863],\n          [-0.6235, -0.5922, -0.5765,  ..., -0.7020, -0.7098, -0.7255],\n          [-0.7176, -0.6941, -0.6627,  ..., -0.3725, -0.4824, -0.3490],\n          ...,\n          [-0.2941, -0.2314, -0.2941,  ..., -0.2000, -0.2863, -0.2549],\n          [-0.2314, -0.2627, -0.4588,  ..., -0.2549, -0.3020, -0.2471],\n          [-0.2157, -0.3020, -0.4275,  ..., -0.2314, -0.2314, -0.2235]],\n\n         [[-0.6471, -0.6157, -0.5843,  ..., -0.7020, -0.7020, -0.6863],\n          [-0.6314, -0.6078, -0.6000,  ..., -0.6627, -0.6784, -0.6941],\n          [-0.6941, -0.6627, -0.6471,  ..., -0.2863, -0.4039, -0.2706],\n          ...,\n          [-0.4039, -0.3412, -0.3725,  ..., -0.3020, -0.3647, -0.3255],\n          [-0.3569, -0.3647, -0.5451,  ..., -0.3255, -0.3569, -0.2941],\n          [-0.3333, -0.3961, -0.5059,  ..., -0.3333, -0.3020, -0.2627]],\n\n         [[-0.6706, -0.6392, -0.6078,  ..., -0.7098, -0.7098, -0.6941],\n          [-0.6471, -0.6392, -0.6157,  ..., -0.6392, -0.6549, -0.6706],\n          [-0.7098, -0.6863, -0.6549,  ..., -0.3020, -0.3961, -0.2784],\n          ...,\n          [-0.5529, -0.4980, -0.5294,  ..., -0.4431, -0.4667, -0.4824],\n          [-0.4824, -0.5216, -0.6392,  ..., -0.4588, -0.4824, -0.4275],\n          [-0.4588, -0.5216, -0.6157,  ..., -0.4510, -0.4431, -0.4118]]],\n\n\n        [[[ 0.4745,  0.4196,  0.3804,  ..., -0.1529, -0.3176,  0.0510],\n          [ 0.6549,  0.6235,  0.4431,  ...,  0.1922, -0.3020, -0.1216],\n          [ 0.8510,  0.7647,  0.3490,  ...,  0.4588, -0.1294, -0.2706],\n          ...,\n          [ 0.3020,  0.0824, -0.0667,  ...,  0.4980,  0.4824,  0.4667],\n          [ 0.0353, -0.1608, -0.3098,  ...,  0.5216,  0.4980,  0.4824],\n          [ 0.4510,  0.3882,  0.3176,  ...,  0.4667,  0.4745,  0.4745]],\n\n         [[ 0.5373,  0.4902,  0.4588,  ..., -0.0039, -0.0275,  0.2549],\n          [ 0.6627,  0.6549,  0.5451,  ...,  0.3098, -0.0353,  0.1059],\n          [ 0.8431,  0.7804,  0.4431,  ...,  0.4902,  0.0588,  0.0275],\n          ...,\n          [ 0.3333,  0.1765,  0.0431,  ...,  0.4667,  0.4431,  0.4353],\n          [ 0.1294, -0.0118, -0.1451,  ...,  0.4745,  0.4431,  0.4431],\n          [ 0.4745,  0.4118,  0.3412,  ...,  0.4353,  0.4431,  0.4431]],\n\n         [[ 0.4667,  0.4275,  0.3569,  ..., -0.0902, -0.2392,  0.1059],\n          [ 0.5765,  0.5608,  0.3961,  ...,  0.2627, -0.2157, -0.0667],\n          [ 0.7255,  0.6706,  0.2941,  ...,  0.4902, -0.0902, -0.2157],\n          ...,\n          [ 0.3333,  0.2157,  0.0902,  ...,  0.4275,  0.4275,  0.4196],\n          [ 0.1686,  0.0510, -0.0902,  ...,  0.4196,  0.4275,  0.4275],\n          [ 0.4510,  0.3882,  0.3098,  ...,  0.4039,  0.4196,  0.4196]]],\n\n\n        [[[-0.9843, -0.9843, -0.9843,  ..., -0.4588, -0.8745, -1.0000],\n          [-0.9843, -0.9843, -1.0000,  ..., -0.6549, -0.9686, -1.0000],\n          [-0.9922, -0.9843, -0.9922,  ..., -0.8902, -1.0000, -0.9922],\n          ...,\n          [-0.6471, -0.6471, -0.5608,  ...,  0.1137, -0.5765, -0.5608],\n          [-0.7176, -0.7020, -0.5765,  ..., -0.2549, -0.5059, -0.5373],\n          [-0.7255, -0.6471, -0.5608,  ..., -0.4745, -0.4824, -0.5059]],\n\n         [[-0.9843, -0.9686, -0.9451,  ..., -0.3333, -0.8118, -0.9843],\n          [-0.9686, -0.9451, -0.9451,  ..., -0.5608, -0.9294, -0.9922],\n          [-0.9608, -0.9137, -0.9059,  ..., -0.8196, -0.9765, -0.9922],\n          ...,\n          [-0.4980, -0.4510, -0.4118,  ...,  0.1765, -0.4667, -0.4196],\n          [-0.5608, -0.4980, -0.4196,  ..., -0.1608, -0.4039, -0.4118],\n          [-0.5686, -0.4510, -0.3961,  ..., -0.3569, -0.3725, -0.3725]],\n\n         [[-0.9843, -0.9922, -1.0000,  ..., -0.8118, -0.9765, -0.9765],\n          [-0.9843, -0.9765, -0.9843,  ..., -0.8667, -0.9922, -0.9765],\n          [-0.9922, -0.9686, -0.9686,  ..., -0.9686, -0.9843, -0.9686],\n          ...,\n          [-0.7647, -0.8196, -0.8039,  ..., -0.1137, -0.7333, -0.7490],\n          [-0.8588, -0.8902, -0.8510,  ..., -0.5059, -0.7255, -0.7804],\n          [-0.8902, -0.8667, -0.8431,  ..., -0.7647, -0.7490, -0.7961]]],\n\n\n        ...,\n\n\n        [[[ 0.7725,  0.7569,  0.7647,  ...,  0.7647,  0.7647,  0.7647],\n          [ 0.8275,  0.8118,  0.8196,  ...,  0.8196,  0.8196,  0.8196],\n          [ 0.9059,  0.8902,  0.8902,  ...,  0.8824,  0.8824,  0.8824],\n          ...,\n          [-0.2471, -0.2471, -0.2471,  ..., -0.1451, -0.2235, -0.3176],\n          [-0.2471, -0.2235, -0.1765,  ..., -0.2157, -0.2941, -0.3333],\n          [-0.2471, -0.3176, -0.3490,  ..., -0.3333, -0.3176, -0.3333]],\n\n         [[ 0.9294,  0.9137,  0.9137,  ...,  0.9137,  0.9137,  0.9137],\n          [ 0.9373,  0.9216,  0.9216,  ...,  0.9216,  0.9216,  0.9216],\n          [ 0.9765,  0.9608,  0.9608,  ...,  0.9608,  0.9608,  0.9608],\n          ...,\n          [-0.0275, -0.0510, -0.0824,  ...,  0.0980,  0.0353, -0.0510],\n          [-0.0431, -0.0431, -0.0196,  ...,  0.0510, -0.0353, -0.0902],\n          [ 0.0196, -0.0745, -0.1294,  ..., -0.0353, -0.0588, -0.1137]],\n\n         [[ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9922,  1.0000],\n          [ 1.0000,  0.9765,  0.9843,  ...,  0.9608,  0.9686,  0.9843],\n          [ 1.0000,  0.9843,  0.9843,  ...,  0.9686,  0.9765,  0.9922],\n          ...,\n          [-0.4275, -0.4275, -0.4275,  ..., -0.3333, -0.4039, -0.5059],\n          [-0.4588, -0.4275, -0.3804,  ..., -0.3804, -0.4745, -0.5373],\n          [-0.4431, -0.5059, -0.5373,  ..., -0.4824, -0.4980, -0.5529]]],\n\n\n        [[[-0.1922, -0.2078, -0.0510,  ...,  0.0196, -0.0745, -0.1608],\n          [ 0.2000,  0.2157,  0.1529,  ..., -0.0510, -0.0824, -0.2000],\n          [ 0.1843,  0.1451,  0.1216,  ..., -0.2706, -0.2392, -0.2392],\n          ...,\n          [ 0.7882,  0.8510,  0.8275,  ...,  0.9059,  0.9294,  0.9451],\n          [ 0.9686,  0.9294,  0.9373,  ...,  0.8118,  0.8275,  0.7804],\n          [ 0.9529,  0.8667,  0.9059,  ...,  0.5843,  0.6392,  0.6706]],\n\n         [[-0.0980, -0.1373, -0.0118,  ...,  0.1216,  0.0431, -0.0588],\n          [ 0.1922,  0.2078,  0.1451,  ...,  0.0667,  0.0510, -0.0745],\n          [ 0.1765,  0.1686,  0.1608,  ..., -0.1373, -0.0980, -0.1059],\n          ...,\n          [ 0.7647,  0.8275,  0.8039,  ...,  0.8824,  0.9059,  0.9216],\n          [ 0.9529,  0.9137,  0.9216,  ...,  0.7961,  0.8118,  0.7647],\n          [ 0.9529,  0.8667,  0.9059,  ...,  0.5843,  0.6392,  0.6706]],\n\n         [[-0.5059, -0.4510, -0.1686,  ..., -0.2000, -0.3020, -0.3412],\n          [-0.2000, -0.0980, -0.0196,  ..., -0.1922, -0.2471, -0.3412],\n          [-0.1765, -0.1137,  0.0196,  ..., -0.3333, -0.3412, -0.3412],\n          ...,\n          [ 0.7569,  0.8431,  0.8353,  ...,  0.8902,  0.9137,  0.9294],\n          [ 0.9529,  0.9216,  0.9373,  ...,  0.8039,  0.8196,  0.7647],\n          [ 0.9529,  0.8667,  0.9059,  ...,  0.5843,  0.6392,  0.6706]]],\n\n\n        [[[-0.4588, -0.2627, -0.1294,  ..., -0.5451, -0.6078, -0.6627],\n          [-0.4118, -0.2157, -0.0902,  ..., -0.4824, -0.5294, -0.5608],\n          [-0.3647, -0.1843, -0.0667,  ..., -0.4196, -0.4431, -0.4588],\n          ...,\n          [-0.4431, -0.3647, -0.3333,  ..., -0.7176, -0.7804, -0.8275],\n          [-0.3490, -0.3176, -0.2784,  ..., -0.7569, -0.7569, -0.8275],\n          [-0.3255, -0.2784, -0.2235,  ..., -0.7725, -0.7961, -0.8667]],\n\n         [[-0.3569, -0.2235, -0.0902,  ..., -0.2706, -0.3176, -0.3569],\n          [-0.3098, -0.1922, -0.0667,  ..., -0.2078, -0.2392, -0.2549],\n          [-0.2784, -0.1843, -0.0588,  ..., -0.1451, -0.1529, -0.1529],\n          ...,\n          [-0.4510, -0.3882, -0.3569,  ..., -0.5529, -0.6000, -0.6314],\n          [-0.3647, -0.3333, -0.2941,  ..., -0.6078, -0.5843, -0.6392],\n          [-0.3333, -0.2941, -0.2549,  ..., -0.6392, -0.6392, -0.6863]],\n\n         [[-0.6000, -0.5765, -0.5059,  ..., -0.8118, -0.8118, -0.8118],\n          [-0.5843, -0.5608, -0.4824,  ..., -0.8353, -0.8353, -0.8275],\n          [-0.5843, -0.5608, -0.4667,  ..., -0.8431, -0.8353, -0.8275],\n          ...,\n          [-0.6471, -0.6471, -0.6784,  ..., -0.7490, -0.7490, -0.7647],\n          [-0.6392, -0.6706, -0.6863,  ..., -0.7804, -0.7333, -0.7647],\n          [-0.6941, -0.7176, -0.6941,  ..., -0.7961, -0.7882, -0.8196]]]],\n       device='cuda:0')\u001b[0m\n\u001b[1;32m     64\u001b[0m       \u001b[0mval_loss_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m       \u001b[0mtotal_val_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mval_loss_size\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self=SimpleCNN(\n  (conv1): Conv2d(3, 256, kernel_size...ar(in_features=128, out_features=10, bias=True)\n), *input=(tensor([[[[-0.6235, -0.5843, -0.5529,  ..., -0.7...61, -0.7882, -0.8196]]]],\n       device='cuda:0'),), **kwargs={})\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mresult\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.forward\u001b[0m \u001b[0;34m= <bound method SimpleCNN.forward of SimpleCNN(\n  (conv1): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout1): Dropout(p=0.5)\n  (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout2): Dropout(p=0.5)\n  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout3): Dropout(p=0.5)\n  (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout4): Dropout(p=0.5)\n  (pool4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (conv5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout5): Dropout(p=0.5)\n  (conv6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout6): Dropout(p=0.5)\n  (conv7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout7): Dropout(p=0.5)\n  (conv8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (bn8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout8): Dropout(p=0.5)\n  (conv9): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  (bn9): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout9): Dropout(p=0.5)\n  (conv10): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  (bn10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout10): Dropout(p=0.5)\n  (conv11): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  (bn11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout11): Dropout(p=0.5)\n  (conv12): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n  (bn12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (dropout12): Dropout(p=0.5)\n  (fc1): Linear(in_features=4096, out_features=128, bias=True)\n  (dropoutfc1): Dropout(p=0.3)\n  (fc2): Linear(in_features=128, out_features=10, bias=True)\n)>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36minput\u001b[0m \u001b[0;34m= (tensor([[[[-0.6235, -0.5843, -0.5529,  ..., -0.7020, -0.7020, -0.6863],\n          [-0.6235, -0.5922, -0.5765,  ..., -0.7020, -0.7098, -0.7255],\n          [-0.7176, -0.6941, -0.6627,  ..., -0.3725, -0.4824, -0.3490],\n          ...,\n          [-0.2941, -0.2314, -0.2941,  ..., -0.2000, -0.2863, -0.2549],\n          [-0.2314, -0.2627, -0.4588,  ..., -0.2549, -0.3020, -0.2471],\n          [-0.2157, -0.3020, -0.4275,  ..., -0.2314, -0.2314, -0.2235]],\n\n         [[-0.6471, -0.6157, -0.5843,  ..., -0.7020, -0.7020, -0.6863],\n          [-0.6314, -0.6078, -0.6000,  ..., -0.6627, -0.6784, -0.6941],\n          [-0.6941, -0.6627, -0.6471,  ..., -0.2863, -0.4039, -0.2706],\n          ...,\n          [-0.4039, -0.3412, -0.3725,  ..., -0.3020, -0.3647, -0.3255],\n          [-0.3569, -0.3647, -0.5451,  ..., -0.3255, -0.3569, -0.2941],\n          [-0.3333, -0.3961, -0.5059,  ..., -0.3333, -0.3020, -0.2627]],\n\n         [[-0.6706, -0.6392, -0.6078,  ..., -0.7098, -0.7098, -0.6941],\n          [-0.6471, -0.6392, -0.6157,  ..., -0.6392, -0.6549, -0.6706],\n          [-0.7098, -0.6863, -0.6549,  ..., -0.3020, -0.3961, -0.2784],\n          ...,\n          [-0.5529, -0.4980, -0.5294,  ..., -0.4431, -0.4667, -0.4824],\n          [-0.4824, -0.5216, -0.6392,  ..., -0.4588, -0.4824, -0.4275],\n          [-0.4588, -0.5216, -0.6157,  ..., -0.4510, -0.4431, -0.4118]]],\n\n\n        [[[ 0.4745,  0.4196,  0.3804,  ..., -0.1529, -0.3176,  0.0510],\n          [ 0.6549,  0.6235,  0.4431,  ...,  0.1922, -0.3020, -0.1216],\n          [ 0.8510,  0.7647,  0.3490,  ...,  0.4588, -0.1294, -0.2706],\n          ...,\n          [ 0.3020,  0.0824, -0.0667,  ...,  0.4980,  0.4824,  0.4667],\n          [ 0.0353, -0.1608, -0.3098,  ...,  0.5216,  0.4980,  0.4824],\n          [ 0.4510,  0.3882,  0.3176,  ...,  0.4667,  0.4745,  0.4745]],\n\n         [[ 0.5373,  0.4902,  0.4588,  ..., -0.0039, -0.0275,  0.2549],\n          [ 0.6627,  0.6549,  0.5451,  ...,  0.3098, -0.0353,  0.1059],\n          [ 0.8431,  0.7804,  0.4431,  ...,  0.4902,  0.0588,  0.0275],\n          ...,\n          [ 0.3333,  0.1765,  0.0431,  ...,  0.4667,  0.4431,  0.4353],\n          [ 0.1294, -0.0118, -0.1451,  ...,  0.4745,  0.4431,  0.4431],\n          [ 0.4745,  0.4118,  0.3412,  ...,  0.4353,  0.4431,  0.4431]],\n\n         [[ 0.4667,  0.4275,  0.3569,  ..., -0.0902, -0.2392,  0.1059],\n          [ 0.5765,  0.5608,  0.3961,  ...,  0.2627, -0.2157, -0.0667],\n          [ 0.7255,  0.6706,  0.2941,  ...,  0.4902, -0.0902, -0.2157],\n          ...,\n          [ 0.3333,  0.2157,  0.0902,  ...,  0.4275,  0.4275,  0.4196],\n          [ 0.1686,  0.0510, -0.0902,  ...,  0.4196,  0.4275,  0.4275],\n          [ 0.4510,  0.3882,  0.3098,  ...,  0.4039,  0.4196,  0.4196]]],\n\n\n        [[[-0.9843, -0.9843, -0.9843,  ..., -0.4588, -0.8745, -1.0000],\n          [-0.9843, -0.9843, -1.0000,  ..., -0.6549, -0.9686, -1.0000],\n          [-0.9922, -0.9843, -0.9922,  ..., -0.8902, -1.0000, -0.9922],\n          ...,\n          [-0.6471, -0.6471, -0.5608,  ...,  0.1137, -0.5765, -0.5608],\n          [-0.7176, -0.7020, -0.5765,  ..., -0.2549, -0.5059, -0.5373],\n          [-0.7255, -0.6471, -0.5608,  ..., -0.4745, -0.4824, -0.5059]],\n\n         [[-0.9843, -0.9686, -0.9451,  ..., -0.3333, -0.8118, -0.9843],\n          [-0.9686, -0.9451, -0.9451,  ..., -0.5608, -0.9294, -0.9922],\n          [-0.9608, -0.9137, -0.9059,  ..., -0.8196, -0.9765, -0.9922],\n          ...,\n          [-0.4980, -0.4510, -0.4118,  ...,  0.1765, -0.4667, -0.4196],\n          [-0.5608, -0.4980, -0.4196,  ..., -0.1608, -0.4039, -0.4118],\n          [-0.5686, -0.4510, -0.3961,  ..., -0.3569, -0.3725, -0.3725]],\n\n         [[-0.9843, -0.9922, -1.0000,  ..., -0.8118, -0.9765, -0.9765],\n          [-0.9843, -0.9765, -0.9843,  ..., -0.8667, -0.9922, -0.9765],\n          [-0.9922, -0.9686, -0.9686,  ..., -0.9686, -0.9843, -0.9686],\n          ...,\n          [-0.7647, -0.8196, -0.8039,  ..., -0.1137, -0.7333, -0.7490],\n          [-0.8588, -0.8902, -0.8510,  ..., -0.5059, -0.7255, -0.7804],\n          [-0.8902, -0.8667, -0.8431,  ..., -0.7647, -0.7490, -0.7961]]],\n\n\n        ...,\n\n\n        [[[ 0.7725,  0.7569,  0.7647,  ...,  0.7647,  0.7647,  0.7647],\n          [ 0.8275,  0.8118,  0.8196,  ...,  0.8196,  0.8196,  0.8196],\n          [ 0.9059,  0.8902,  0.8902,  ...,  0.8824,  0.8824,  0.8824],\n          ...,\n          [-0.2471, -0.2471, -0.2471,  ..., -0.1451, -0.2235, -0.3176],\n          [-0.2471, -0.2235, -0.1765,  ..., -0.2157, -0.2941, -0.3333],\n          [-0.2471, -0.3176, -0.3490,  ..., -0.3333, -0.3176, -0.3333]],\n\n         [[ 0.9294,  0.9137,  0.9137,  ...,  0.9137,  0.9137,  0.9137],\n          [ 0.9373,  0.9216,  0.9216,  ...,  0.9216,  0.9216,  0.9216],\n          [ 0.9765,  0.9608,  0.9608,  ...,  0.9608,  0.9608,  0.9608],\n          ...,\n          [-0.0275, -0.0510, -0.0824,  ...,  0.0980,  0.0353, -0.0510],\n          [-0.0431, -0.0431, -0.0196,  ...,  0.0510, -0.0353, -0.0902],\n          [ 0.0196, -0.0745, -0.1294,  ..., -0.0353, -0.0588, -0.1137]],\n\n         [[ 1.0000,  1.0000,  1.0000,  ...,  0.9843,  0.9922,  1.0000],\n          [ 1.0000,  0.9765,  0.9843,  ...,  0.9608,  0.9686,  0.9843],\n          [ 1.0000,  0.9843,  0.9843,  ...,  0.9686,  0.9765,  0.9922],\n          ...,\n          [-0.4275, -0.4275, -0.4275,  ..., -0.3333, -0.4039, -0.5059],\n          [-0.4588, -0.4275, -0.3804,  ..., -0.3804, -0.4745, -0.5373],\n          [-0.4431, -0.5059, -0.5373,  ..., -0.4824, -0.4980, -0.5529]]],\n\n\n        [[[-0.1922, -0.2078, -0.0510,  ...,  0.0196, -0.0745, -0.1608],\n          [ 0.2000,  0.2157,  0.1529,  ..., -0.0510, -0.0824, -0.2000],\n          [ 0.1843,  0.1451,  0.1216,  ..., -0.2706, -0.2392, -0.2392],\n          ...,\n          [ 0.7882,  0.8510,  0.8275,  ...,  0.9059,  0.9294,  0.9451],\n          [ 0.9686,  0.9294,  0.9373,  ...,  0.8118,  0.8275,  0.7804],\n          [ 0.9529,  0.8667,  0.9059,  ...,  0.5843,  0.6392,  0.6706]],\n\n         [[-0.0980, -0.1373, -0.0118,  ...,  0.1216,  0.0431, -0.0588],\n          [ 0.1922,  0.2078,  0.1451,  ...,  0.0667,  0.0510, -0.0745],\n          [ 0.1765,  0.1686,  0.1608,  ..., -0.1373, -0.0980, -0.1059],\n          ...,\n          [ 0.7647,  0.8275,  0.8039,  ...,  0.8824,  0.9059,  0.9216],\n          [ 0.9529,  0.9137,  0.9216,  ...,  0.7961,  0.8118,  0.7647],\n          [ 0.9529,  0.8667,  0.9059,  ...,  0.5843,  0.6392,  0.6706]],\n\n         [[-0.5059, -0.4510, -0.1686,  ..., -0.2000, -0.3020, -0.3412],\n          [-0.2000, -0.0980, -0.0196,  ..., -0.1922, -0.2471, -0.3412],\n          [-0.1765, -0.1137,  0.0196,  ..., -0.3333, -0.3412, -0.3412],\n          ...,\n          [ 0.7569,  0.8431,  0.8353,  ...,  0.8902,  0.9137,  0.9294],\n          [ 0.9529,  0.9216,  0.9373,  ...,  0.8039,  0.8196,  0.7647],\n          [ 0.9529,  0.8667,  0.9059,  ...,  0.5843,  0.6392,  0.6706]]],\n\n\n        [[[-0.4588, -0.2627, -0.1294,  ..., -0.5451, -0.6078, -0.6627],\n          [-0.4118, -0.2157, -0.0902,  ..., -0.4824, -0.5294, -0.5608],\n          [-0.3647, -0.1843, -0.0667,  ..., -0.4196, -0.4431, -0.4588],\n          ...,\n          [-0.4431, -0.3647, -0.3333,  ..., -0.7176, -0.7804, -0.8275],\n          [-0.3490, -0.3176, -0.2784,  ..., -0.7569, -0.7569, -0.8275],\n          [-0.3255, -0.2784, -0.2235,  ..., -0.7725, -0.7961, -0.8667]],\n\n         [[-0.3569, -0.2235, -0.0902,  ..., -0.2706, -0.3176, -0.3569],\n          [-0.3098, -0.1922, -0.0667,  ..., -0.2078, -0.2392, -0.2549],\n          [-0.2784, -0.1843, -0.0588,  ..., -0.1451, -0.1529, -0.1529],\n          ...,\n          [-0.4510, -0.3882, -0.3569,  ..., -0.5529, -0.6000, -0.6314],\n          [-0.3647, -0.3333, -0.2941,  ..., -0.6078, -0.5843, -0.6392],\n          [-0.3333, -0.2941, -0.2549,  ..., -0.6392, -0.6392, -0.6863]],\n\n         [[-0.6000, -0.5765, -0.5059,  ..., -0.8118, -0.8118, -0.8118],\n          [-0.5843, -0.5608, -0.4824,  ..., -0.8353, -0.8353, -0.8275],\n          [-0.5843, -0.5608, -0.4667,  ..., -0.8431, -0.8353, -0.8275],\n          ...,\n          [-0.6471, -0.6471, -0.6784,  ..., -0.7490, -0.7490, -0.7647],\n          [-0.6392, -0.6706, -0.6863,  ..., -0.7804, -0.7333, -0.7647],\n          [-0.6941, -0.7176, -0.6941,  ..., -0.7961, -0.7882, -0.8196]]]],\n       device='cuda:0'),)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mkwargs\u001b[0m \u001b[0;34m= {}\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-41e7689e9f1a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self=SimpleCNN(\n  (conv1): Conv2d(3, 256, kernel_size...ar(in_features=128, out_features=10, bias=True)\n), x=tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000,...      device='cuda:0', grad_fn=<DropoutBackward>))\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mx\u001b[0m \u001b[0;34m= tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.2118, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1543],\n          [0.0829, 0.1000, 0.1026,  ..., 0.6490, 0.8328, 0.2323],\n          [0.3103, 0.1381, 0.0682,  ..., 0.3972, 0.4399, 0.1032],\n          ...,\n          [0.1012, 0.0000, 0.0000,  ..., 0.1556, 0.2691, 0.1398],\n          [0.0000, 0.1156, 0.3524,  ..., 0.2372, 0.1737, 0.0998],\n          [0.5152, 0.8974, 1.1897,  ..., 0.6292, 0.5150, 0.0591]],\n\n         [[0.1518, 0.0912, 0.0971,  ..., 0.1470, 0.1381, 0.0000],\n          [0.2749, 0.0000, 0.0000,  ..., 0.0375, 0.0235, 0.0000],\n          [0.3102, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.2751, 0.0130, 0.0399,  ..., 0.0094, 0.0154, 0.0000],\n          [0.2720, 0.0867, 0.2307,  ..., 0.0248, 0.0392, 0.0000],\n          [0.4426, 0.3050, 0.3120,  ..., 0.1701, 0.1451, 0.0000]],\n\n         ...,\n\n         [[0.1136, 0.3965, 0.3693,  ..., 0.5017, 0.4818, 0.3694],\n          [0.1254, 0.1176, 0.1115,  ..., 0.1424, 0.1622, 0.1474],\n          [0.1965, 0.1775, 0.1268,  ..., 0.0000, 0.0185, 0.0930],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0599, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.1969, 0.1007, 0.0984,  ..., 0.1286, 0.1372, 0.0000],\n          [0.1421, 0.0750, 0.0671,  ..., 0.0300, 0.0000, 0.0000],\n          [0.1263, 0.0717, 0.0870,  ..., 0.0785, 0.0157, 0.0000],\n          ...,\n          [0.0258, 0.0583, 0.1607,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0394, 0.0601, 0.0711,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.1174, 0.1100,  ..., 0.1400, 0.1308, 0.1423],\n          [0.0000, 0.1510, 0.1518,  ..., 0.3665, 0.3249, 0.3085],\n          [0.0000, 0.2177, 0.1868,  ..., 0.0048, 0.0072, 0.0761],\n          ...,\n          [0.0000, 0.1472, 0.0939,  ..., 0.1561, 0.1198, 0.1505],\n          [0.0000, 0.1454, 0.1081,  ..., 0.1748, 0.1396, 0.1570],\n          [0.1720, 0.3554, 0.3672,  ..., 0.3471, 0.3320, 0.2740]]],\n\n\n        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0286, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0052,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0214, 0.3579, 0.0000,  ..., 0.5094, 0.2290, 0.0000],\n          [0.0212, 0.0000, 0.2525,  ..., 0.0000, 0.3337, 0.5300],\n          [0.0000, 0.0000, 1.0706,  ..., 0.0772, 0.0495, 0.7131],\n          ...,\n          [0.0000, 0.0000, 0.1859,  ..., 0.0404, 0.3611, 0.0000],\n          [0.4954, 0.5435, 1.2207,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.1504,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0685, 0.1127, 0.1480,  ..., 0.3035, 0.0000, 0.0868],\n          [0.0000, 0.2570, 0.3435,  ..., 0.6777, 0.0000, 0.0000],\n          [0.0000, 0.2939, 0.3804,  ..., 0.6603, 0.3152, 0.0000],\n          ...,\n          [0.0304, 0.3361, 0.7652,  ..., 0.0000, 0.0000, 0.4241],\n          [0.1819, 0.3270, 0.4508,  ..., 0.1965, 0.1994, 0.4594],\n          [0.0000, 0.0232, 0.0513,  ..., 0.1274, 0.1051, 0.3653]],\n\n         ...,\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1108],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0107],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0066, 0.1043,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0087, 0.1207, 0.1662,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.3152, 0.1634, 0.1227,  ..., 0.0414, 0.1245, 0.1001],\n          [0.4536, 0.0970, 0.0640,  ..., 0.0000, 0.0470, 0.1167],\n          [0.2922, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0358],\n          ...,\n          [0.2215, 0.0000, 0.0000,  ..., 0.1369, 0.2411, 0.0921],\n          [0.3270, 0.1974, 0.2813,  ..., 0.0372, 0.0380, 0.0580],\n          [0.0570, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n\n\n        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.3027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3332],\n          [0.3766, 0.4341, 0.4502,  ..., 0.4916, 0.3019, 0.2540],\n          [0.4066, 0.4938, 0.6641,  ..., 0.5029, 0.2347, 0.2934],\n          ...,\n          [0.4480, 0.8209, 1.0566,  ..., 0.4239, 0.7543, 0.7076],\n          [0.7123, 0.7972, 0.9326,  ..., 0.6262, 0.2793, 0.6673],\n          [1.3950, 1.3639, 1.1510,  ..., 0.7201, 0.9932, 0.2295]],\n\n         [[0.2018, 0.1281, 0.1332,  ..., 0.1649, 0.2113, 0.0000],\n          [0.3950, 0.0000, 0.0000,  ..., 0.1432, 0.0906, 0.0000],\n          [0.3895, 0.0000, 0.0000,  ..., 0.0978, 0.0000, 0.0000],\n          ...,\n          [0.2903, 0.0000, 0.0000,  ..., 0.5679, 0.1391, 0.0000],\n          [0.3115, 0.0000, 0.0000,  ..., 0.5339, 0.2473, 0.0000],\n          [0.5096, 0.0822, 0.0328,  ..., 0.5645, 0.2826, 0.0000]],\n\n         ...,\n\n         [[0.2303, 0.6619, 0.6871,  ..., 0.5137, 0.6606, 0.5694],\n          [0.2869, 0.2608, 0.2743,  ..., 0.1411, 0.3558, 0.2653],\n          [0.2688, 0.2444, 0.2736,  ..., 0.2721, 0.3707, 0.2497],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1331],\n          [0.0000, 0.0031, 0.0000,  ..., 0.0920, 0.1899, 0.2197],\n          [0.1957, 0.0000, 0.0000,  ..., 0.1818, 0.0000, 0.0000]],\n\n         [[0.3950, 0.2599, 0.2583,  ..., 0.2952, 0.3387, 0.0000],\n          [0.2593, 0.1781, 0.1863,  ..., 0.1541, 0.1521, 0.0025],\n          [0.2580, 0.1752, 0.1620,  ..., 0.1548, 0.1513, 0.0127],\n          ...,\n          [0.1232, 0.0663, 0.0034,  ..., 0.0000, 0.0869, 0.0000],\n          [0.1306, 0.0559, 0.0027,  ..., 0.0000, 0.0273, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.1300, 0.1194,  ..., 0.0000, 0.0000, 0.0931],\n          [0.0000, 0.2242, 0.2019,  ..., 0.0000, 0.0000, 0.1346],\n          [0.0000, 0.2219, 0.2009,  ..., 0.0000, 0.0352, 0.1856],\n          ...,\n          [0.0000, 0.1883, 0.2049,  ..., 0.0000, 0.0000, 0.1277],\n          [0.0000, 0.2136, 0.2086,  ..., 0.0000, 0.0000, 0.0732],\n          [0.2296, 0.6278, 0.5699,  ..., 0.2606, 0.4423, 0.3351]]],\n\n\n        ...,\n\n\n        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0663, 0.0249, 0.0248,  ..., 0.0271, 0.0269, 0.0000],\n          [0.0755, 0.0402, 0.0438,  ..., 0.0404, 0.0398, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.2685, 0.2504,  ..., 0.2740, 0.2681, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.6721, 0.8229, 0.6015,  ..., 0.5704, 0.7032, 0.6997],\n          [0.5175, 0.4852, 0.3641,  ..., 0.6505, 0.6847, 0.5931],\n          [0.6388, 0.6975, 0.7389,  ..., 0.7340, 0.7572, 0.1912]],\n\n         [[0.0300, 0.0931, 0.0907,  ..., 0.0850, 0.0856, 0.3916],\n          [0.0000, 0.2128, 0.2135,  ..., 0.2143, 0.2117, 0.8159],\n          [0.0000, 0.2127, 0.2134,  ..., 0.2272, 0.2195, 0.8295],\n          ...,\n          [0.1963, 0.0603, 0.0097,  ..., 0.0000, 0.0218, 0.0000],\n          [0.2297, 0.0000, 0.0000,  ..., 0.0500, 0.0982, 0.0000],\n          [0.2734, 0.1543, 0.1766,  ..., 0.1586, 0.1885, 0.0000]],\n\n         ...,\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2342],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2612],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.3677, 0.1168, 0.1230,  ..., 0.1183, 0.1190, 0.0709],\n          [0.4914, 0.0164, 0.0277,  ..., 0.0250, 0.0232, 0.0262],\n          [0.5032, 0.0000, 0.0109,  ..., 0.0259, 0.0223, 0.0245],\n          ...,\n          [0.1171, 0.1031, 0.1033,  ..., 0.1307, 0.0847, 0.0000],\n          [0.1078, 0.0792, 0.0563,  ..., 0.0837, 0.0705, 0.0000],\n          [0.2702, 0.3186, 0.3551,  ..., 0.3422, 0.3334, 0.1871]]],\n\n\n        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0521, 0.0189, 0.0306,  ..., 0.0302, 0.0373, 0.0000],\n          [0.0584, 0.0439, 0.0412,  ..., 0.0355, 0.0364, 0.0000],\n          [0.0000, 0.0455, 0.0355,  ..., 0.0024, 0.0070, 0.0000]],\n\n         [[0.9576, 0.5625, 0.1238,  ..., 0.3111, 0.2235, 0.3289],\n          [0.2416, 0.4128, 0.2245,  ..., 0.0053, 0.1826, 0.3076],\n          [0.0000, 0.0000, 0.0000,  ..., 0.1699, 0.2625, 0.1975],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.1178, 0.0613, 0.0000,  ..., 0.0854, 0.1175, 0.0332],\n          [0.1793, 0.0000, 0.0000,  ..., 0.1460, 0.1620, 0.0000],\n          [0.1065, 0.0643, 0.1144,  ..., 0.1232, 0.1383, 0.0000],\n          ...,\n          [0.0000, 0.1884, 0.2705,  ..., 0.1719, 0.1532, 0.7192],\n          [0.0000, 0.2131, 0.2765,  ..., 0.2240, 0.2444, 0.7912],\n          [0.0000, 0.0861, 0.0908,  ..., 0.1097, 0.1056, 0.5357]],\n\n         ...,\n\n         [[0.0000, 0.1114, 0.0000,  ..., 0.0000, 0.0000, 0.1935],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0766],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1308],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.2126, 0.2280, 0.2120,  ..., 0.0786, 0.0745, 0.0325],\n          [0.1876, 0.0633, 0.0887,  ..., 0.0433, 0.0383, 0.0000],\n          [0.1592, 0.0000, 0.0000,  ..., 0.1249, 0.1428, 0.0249],\n          ...,\n          [0.4965, 0.1089, 0.0962,  ..., 0.0000, 0.0052, 0.0000],\n          [0.4937, 0.0042, 0.0005,  ..., 0.0000, 0.0000, 0.0000],\n          [0.1490, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n\n\n        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.6405, 0.2743, 0.4191,  ..., 0.4126, 0.3733, 0.7551],\n          [0.4713, 0.5327, 0.5231,  ..., 1.2531, 1.3038, 0.9778],\n          [0.4816, 0.5160, 0.4787,  ..., 1.2793, 1.3067, 1.0003],\n          ...,\n          [0.3893, 0.5645, 0.6056,  ..., 0.5668, 0.5395, 0.5099],\n          [0.3780, 0.6522, 0.6539,  ..., 0.4239, 0.4245, 0.5262],\n          [0.8453, 0.8476, 0.7856,  ..., 1.4802, 1.6081, 0.3844]],\n\n         [[0.0508, 0.0000, 0.0000,  ..., 0.0544, 0.0676, 0.0000],\n          [0.2042, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.2059, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.2526, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.2548, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.4465, 0.2117, 0.2364,  ..., 0.0752, 0.1023, 0.0000]],\n\n         ...,\n\n         [[0.0000, 0.0528, 0.0000,  ..., 0.2757, 0.3129, 0.6204],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3877],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3316],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.2065, 0.2472, 0.2727],\n          [0.0000, 0.0000, 0.0000,  ..., 0.2234, 0.2380, 0.2878],\n          [0.0000, 0.0000, 0.0000,  ..., 0.1037, 0.1375, 0.1382]],\n\n         [[0.0710, 0.0000, 0.0000,  ..., 0.1826, 0.1831, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0282, 0.0296, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0049, 0.0000],\n          ...,\n          [0.0527, 0.0351, 0.0368,  ..., 0.0613, 0.0739, 0.0000],\n          [0.0508, 0.0285, 0.0191,  ..., 0.0715, 0.0870, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0149, 0.1650, 0.1515,  ..., 0.0847, 0.0918, 0.0343],\n          [0.0000, 0.2553, 0.2203,  ..., 0.1301, 0.1367, 0.0000],\n          [0.0155, 0.2406, 0.2112,  ..., 0.1579, 0.1583, 0.0000],\n          ...,\n          [0.0000, 0.2121, 0.1907,  ..., 0.1098, 0.1460, 0.0942],\n          [0.0000, 0.1829, 0.1846,  ..., 0.1360, 0.1402, 0.0684],\n          [0.2513, 0.4765, 0.4630,  ..., 0.5718, 0.5658, 0.4251]]]],\n       device='cuda:0', grad_fn=<DropoutBackward>)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mglobal\u001b[0m \u001b[0;36mF.relu\u001b[0m \u001b[0;34m= <function relu at 0x7f5fb66ff268>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.bn2\u001b[0m \u001b[0;34m= BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.conv2\u001b[0m \u001b[0;34m= Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self=Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), *input=(tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000,...      device='cuda:0', grad_fn=<DropoutBackward>),), **kwargs={})\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m        \u001b[0;36mresult\u001b[0m \u001b[0;34m= \u001b[0;36mundefined\u001b[0m\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.forward\u001b[0m \u001b[0;34m= <bound method Conv2d.forward of Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))>\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36minput\u001b[0m \u001b[0;34m= (tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.2118, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1543],\n          [0.0829, 0.1000, 0.1026,  ..., 0.6490, 0.8328, 0.2323],\n          [0.3103, 0.1381, 0.0682,  ..., 0.3972, 0.4399, 0.1032],\n          ...,\n          [0.1012, 0.0000, 0.0000,  ..., 0.1556, 0.2691, 0.1398],\n          [0.0000, 0.1156, 0.3524,  ..., 0.2372, 0.1737, 0.0998],\n          [0.5152, 0.8974, 1.1897,  ..., 0.6292, 0.5150, 0.0591]],\n\n         [[0.1518, 0.0912, 0.0971,  ..., 0.1470, 0.1381, 0.0000],\n          [0.2749, 0.0000, 0.0000,  ..., 0.0375, 0.0235, 0.0000],\n          [0.3102, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.2751, 0.0130, 0.0399,  ..., 0.0094, 0.0154, 0.0000],\n          [0.2720, 0.0867, 0.2307,  ..., 0.0248, 0.0392, 0.0000],\n          [0.4426, 0.3050, 0.3120,  ..., 0.1701, 0.1451, 0.0000]],\n\n         ...,\n\n         [[0.1136, 0.3965, 0.3693,  ..., 0.5017, 0.4818, 0.3694],\n          [0.1254, 0.1176, 0.1115,  ..., 0.1424, 0.1622, 0.1474],\n          [0.1965, 0.1775, 0.1268,  ..., 0.0000, 0.0185, 0.0930],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0599, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.1969, 0.1007, 0.0984,  ..., 0.1286, 0.1372, 0.0000],\n          [0.1421, 0.0750, 0.0671,  ..., 0.0300, 0.0000, 0.0000],\n          [0.1263, 0.0717, 0.0870,  ..., 0.0785, 0.0157, 0.0000],\n          ...,\n          [0.0258, 0.0583, 0.1607,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0394, 0.0601, 0.0711,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.1174, 0.1100,  ..., 0.1400, 0.1308, 0.1423],\n          [0.0000, 0.1510, 0.1518,  ..., 0.3665, 0.3249, 0.3085],\n          [0.0000, 0.2177, 0.1868,  ..., 0.0048, 0.0072, 0.0761],\n          ...,\n          [0.0000, 0.1472, 0.0939,  ..., 0.1561, 0.1198, 0.1505],\n          [0.0000, 0.1454, 0.1081,  ..., 0.1748, 0.1396, 0.1570],\n          [0.1720, 0.3554, 0.3672,  ..., 0.3471, 0.3320, 0.2740]]],\n\n\n        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0286, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0052,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0214, 0.3579, 0.0000,  ..., 0.5094, 0.2290, 0.0000],\n          [0.0212, 0.0000, 0.2525,  ..., 0.0000, 0.3337, 0.5300],\n          [0.0000, 0.0000, 1.0706,  ..., 0.0772, 0.0495, 0.7131],\n          ...,\n          [0.0000, 0.0000, 0.1859,  ..., 0.0404, 0.3611, 0.0000],\n          [0.4954, 0.5435, 1.2207,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.1504,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0685, 0.1127, 0.1480,  ..., 0.3035, 0.0000, 0.0868],\n          [0.0000, 0.2570, 0.3435,  ..., 0.6777, 0.0000, 0.0000],\n          [0.0000, 0.2939, 0.3804,  ..., 0.6603, 0.3152, 0.0000],\n          ...,\n          [0.0304, 0.3361, 0.7652,  ..., 0.0000, 0.0000, 0.4241],\n          [0.1819, 0.3270, 0.4508,  ..., 0.1965, 0.1994, 0.4594],\n          [0.0000, 0.0232, 0.0513,  ..., 0.1274, 0.1051, 0.3653]],\n\n         ...,\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1108],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0107],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0066, 0.1043,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0087, 0.1207, 0.1662,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0105, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.3152, 0.1634, 0.1227,  ..., 0.0414, 0.1245, 0.1001],\n          [0.4536, 0.0970, 0.0640,  ..., 0.0000, 0.0470, 0.1167],\n          [0.2922, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0358],\n          ...,\n          [0.2215, 0.0000, 0.0000,  ..., 0.1369, 0.2411, 0.0921],\n          [0.3270, 0.1974, 0.2813,  ..., 0.0372, 0.0380, 0.0580],\n          [0.0570, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n\n\n        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.3027, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3332],\n          [0.3766, 0.4341, 0.4502,  ..., 0.4916, 0.3019, 0.2540],\n          [0.4066, 0.4938, 0.6641,  ..., 0.5029, 0.2347, 0.2934],\n          ...,\n          [0.4480, 0.8209, 1.0566,  ..., 0.4239, 0.7543, 0.7076],\n          [0.7123, 0.7972, 0.9326,  ..., 0.6262, 0.2793, 0.6673],\n          [1.3950, 1.3639, 1.1510,  ..., 0.7201, 0.9932, 0.2295]],\n\n         [[0.2018, 0.1281, 0.1332,  ..., 0.1649, 0.2113, 0.0000],\n          [0.3950, 0.0000, 0.0000,  ..., 0.1432, 0.0906, 0.0000],\n          [0.3895, 0.0000, 0.0000,  ..., 0.0978, 0.0000, 0.0000],\n          ...,\n          [0.2903, 0.0000, 0.0000,  ..., 0.5679, 0.1391, 0.0000],\n          [0.3115, 0.0000, 0.0000,  ..., 0.5339, 0.2473, 0.0000],\n          [0.5096, 0.0822, 0.0328,  ..., 0.5645, 0.2826, 0.0000]],\n\n         ...,\n\n         [[0.2303, 0.6619, 0.6871,  ..., 0.5137, 0.6606, 0.5694],\n          [0.2869, 0.2608, 0.2743,  ..., 0.1411, 0.3558, 0.2653],\n          [0.2688, 0.2444, 0.2736,  ..., 0.2721, 0.3707, 0.2497],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1331],\n          [0.0000, 0.0031, 0.0000,  ..., 0.0920, 0.1899, 0.2197],\n          [0.1957, 0.0000, 0.0000,  ..., 0.1818, 0.0000, 0.0000]],\n\n         [[0.3950, 0.2599, 0.2583,  ..., 0.2952, 0.3387, 0.0000],\n          [0.2593, 0.1781, 0.1863,  ..., 0.1541, 0.1521, 0.0025],\n          [0.2580, 0.1752, 0.1620,  ..., 0.1548, 0.1513, 0.0127],\n          ...,\n          [0.1232, 0.0663, 0.0034,  ..., 0.0000, 0.0869, 0.0000],\n          [0.1306, 0.0559, 0.0027,  ..., 0.0000, 0.0273, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.1300, 0.1194,  ..., 0.0000, 0.0000, 0.0931],\n          [0.0000, 0.2242, 0.2019,  ..., 0.0000, 0.0000, 0.1346],\n          [0.0000, 0.2219, 0.2009,  ..., 0.0000, 0.0352, 0.1856],\n          ...,\n          [0.0000, 0.1883, 0.2049,  ..., 0.0000, 0.0000, 0.1277],\n          [0.0000, 0.2136, 0.2086,  ..., 0.0000, 0.0000, 0.0732],\n          [0.2296, 0.6278, 0.5699,  ..., 0.2606, 0.4423, 0.3351]]],\n\n\n        ...,\n\n\n        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0663, 0.0249, 0.0248,  ..., 0.0271, 0.0269, 0.0000],\n          [0.0755, 0.0402, 0.0438,  ..., 0.0404, 0.0398, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.2685, 0.2504,  ..., 0.2740, 0.2681, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.6721, 0.8229, 0.6015,  ..., 0.5704, 0.7032, 0.6997],\n          [0.5175, 0.4852, 0.3641,  ..., 0.6505, 0.6847, 0.5931],\n          [0.6388, 0.6975, 0.7389,  ..., 0.7340, 0.7572, 0.1912]],\n\n         [[0.0300, 0.0931, 0.0907,  ..., 0.0850, 0.0856, 0.3916],\n          [0.0000, 0.2128, 0.2135,  ..., 0.2143, 0.2117, 0.8159],\n          [0.0000, 0.2127, 0.2134,  ..., 0.2272, 0.2195, 0.8295],\n          ...,\n          [0.1963, 0.0603, 0.0097,  ..., 0.0000, 0.0218, 0.0000],\n          [0.2297, 0.0000, 0.0000,  ..., 0.0500, 0.0982, 0.0000],\n          [0.2734, 0.1543, 0.1766,  ..., 0.1586, 0.1885, 0.0000]],\n\n         ...,\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2342],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.2612],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.3677, 0.1168, 0.1230,  ..., 0.1183, 0.1190, 0.0709],\n          [0.4914, 0.0164, 0.0277,  ..., 0.0250, 0.0232, 0.0262],\n          [0.5032, 0.0000, 0.0109,  ..., 0.0259, 0.0223, 0.0245],\n          ...,\n          [0.1171, 0.1031, 0.1033,  ..., 0.1307, 0.0847, 0.0000],\n          [0.1078, 0.0792, 0.0563,  ..., 0.0837, 0.0705, 0.0000],\n          [0.2702, 0.3186, 0.3551,  ..., 0.3422, 0.3334, 0.1871]]],\n\n\n        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0521, 0.0189, 0.0306,  ..., 0.0302, 0.0373, 0.0000],\n          [0.0584, 0.0439, 0.0412,  ..., 0.0355, 0.0364, 0.0000],\n          [0.0000, 0.0455, 0.0355,  ..., 0.0024, 0.0070, 0.0000]],\n\n         [[0.9576, 0.5625, 0.1238,  ..., 0.3111, 0.2235, 0.3289],\n          [0.2416, 0.4128, 0.2245,  ..., 0.0053, 0.1826, 0.3076],\n          [0.0000, 0.0000, 0.0000,  ..., 0.1699, 0.2625, 0.1975],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.1178, 0.0613, 0.0000,  ..., 0.0854, 0.1175, 0.0332],\n          [0.1793, 0.0000, 0.0000,  ..., 0.1460, 0.1620, 0.0000],\n          [0.1065, 0.0643, 0.1144,  ..., 0.1232, 0.1383, 0.0000],\n          ...,\n          [0.0000, 0.1884, 0.2705,  ..., 0.1719, 0.1532, 0.7192],\n          [0.0000, 0.2131, 0.2765,  ..., 0.2240, 0.2444, 0.7912],\n          [0.0000, 0.0861, 0.0908,  ..., 0.1097, 0.1056, 0.5357]],\n\n         ...,\n\n         [[0.0000, 0.1114, 0.0000,  ..., 0.0000, 0.0000, 0.1935],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0766],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.1308],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.2126, 0.2280, 0.2120,  ..., 0.0786, 0.0745, 0.0325],\n          [0.1876, 0.0633, 0.0887,  ..., 0.0433, 0.0383, 0.0000],\n          [0.1592, 0.0000, 0.0000,  ..., 0.1249, 0.1428, 0.0249],\n          ...,\n          [0.4965, 0.1089, 0.0962,  ..., 0.0000, 0.0052, 0.0000],\n          [0.4937, 0.0042, 0.0005,  ..., 0.0000, 0.0000, 0.0000],\n          [0.1490, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]],\n\n\n        [[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.6405, 0.2743, 0.4191,  ..., 0.4126, 0.3733, 0.7551],\n          [0.4713, 0.5327, 0.5231,  ..., 1.2531, 1.3038, 0.9778],\n          [0.4816, 0.5160, 0.4787,  ..., 1.2793, 1.3067, 1.0003],\n          ...,\n          [0.3893, 0.5645, 0.6056,  ..., 0.5668, 0.5395, 0.5099],\n          [0.3780, 0.6522, 0.6539,  ..., 0.4239, 0.4245, 0.5262],\n          [0.8453, 0.8476, 0.7856,  ..., 1.4802, 1.6081, 0.3844]],\n\n         [[0.0508, 0.0000, 0.0000,  ..., 0.0544, 0.0676, 0.0000],\n          [0.2042, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.2059, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          ...,\n          [0.2526, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.2548, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n          [0.4465, 0.2117, 0.2364,  ..., 0.0752, 0.1023, 0.0000]],\n\n         ...,\n\n         [[0.0000, 0.0528, 0.0000,  ..., 0.2757, 0.3129, 0.6204],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3877],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.3316],\n          ...,\n          [0.0000, 0.0000, 0.0000,  ..., 0.2065, 0.2472, 0.2727],\n          [0.0000, 0.0000, 0.0000,  ..., 0.2234, 0.2380, 0.2878],\n          [0.0000, 0.0000, 0.0000,  ..., 0.1037, 0.1375, 0.1382]],\n\n         [[0.0710, 0.0000, 0.0000,  ..., 0.1826, 0.1831, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0282, 0.0296, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0015, 0.0049, 0.0000],\n          ...,\n          [0.0527, 0.0351, 0.0368,  ..., 0.0613, 0.0739, 0.0000],\n          [0.0508, 0.0285, 0.0191,  ..., 0.0715, 0.0870, 0.0000],\n          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n\n         [[0.0149, 0.1650, 0.1515,  ..., 0.0847, 0.0918, 0.0343],\n          [0.0000, 0.2553, 0.2203,  ..., 0.1301, 0.1367, 0.0000],\n          [0.0155, 0.2406, 0.2112,  ..., 0.1579, 0.1583, 0.0000],\n          ...,\n          [0.0000, 0.2121, 0.1907,  ..., 0.1098, 0.1460, 0.0942],\n          [0.0000, 0.1829, 0.1846,  ..., 0.1360, 0.1402, 0.0684],\n          [0.2513, 0.4765, 0.4630,  ..., 0.5718, 0.5658, 0.4251]]]],\n       device='cuda:0', grad_fn=<DropoutBackward>),)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mkwargs\u001b[0m \u001b[0;34m= {}\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self=Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), input=tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000,...      device='cuda:0', grad_fn=<DropoutBackward>))\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m        \u001b[0;36mself.padding\u001b[0m \u001b[0;34m= (1, 1)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.dilation\u001b[0m \u001b[0;34m= (1, 1)\u001b[0m\u001b[0;34m\n        \u001b[0m\u001b[0;36mself.groups\u001b[0m \u001b[0;34m= 1\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory"
     ]
    }
   ],
   "source": [
    "CNN = SimpleCNN()\n",
    "CNN = CNN.to(device)\n",
    "trainNet(CNN, batch_size=32, n_epochs=50, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = eval_accuracy(test_loader,CNN)\n",
    "print(\"accuracy={:.2f}%\".format(accuracy*100))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
